{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# mT5-Small (300 million parameters): gs://t5-data/pretrained_models/mt5/small\n",
    "# mT5-Base (580 million parameters): gs://t5-data/pretrained_models/mt5/base\n",
    "# mT5-Large (1.2 billion parameters): gs://t5-data/pretrained_models/mt5/large\n",
    "# mT5-XL (3.7 billion parameters): gs://t5-data/pretrained_models/mt5/xl\n",
    "# mT5-XXL (13 billion parameters): gs://t5-data/pretrained_models/mt5/xxl\n",
    "\n",
    "models = ['google/mt5-small', 'google/mt5-base', 'google/mt5-large', 'google/mt5-xl', 'google/mt5-xxl']\n",
    "models = [ 'facebook/nllb-200-distilled-600M', 'facebook/nllb-200-distilled-1.3B','facebook/nllb-200-1.3B', 'facebook/nllb-200-3.3B']\n",
    "\n",
    "models = ['facebook/nllb-200-distilled-600M', 'google/mt5-base']\n",
    "\n",
    "for m in models:\n",
    "  tokenizer = AutoTokenizer.from_pretrained(m)\n",
    "  model = AutoModelForSeq2SeqLM.from_pretrained(m)\n",
    "  os.makedirs('./{}'.format(m), exist_ok=True)\n",
    "\n",
    "  tokenizer.save_pretrained('./{}'.format(m))\n",
    "  model.save_pretrained('./{}'.format(m))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
