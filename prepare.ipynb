{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all opensubtitles\n",
    "\n",
    "import zipfile, glob, shutil, gzip\n",
    "from pathlib import Path\n",
    "\n",
    "root = './open/*'\n",
    "e = './extracted/'\n",
    "paths = glob.glob(root)\n",
    "\n",
    "for p in paths:\n",
    "    if p.split('.')[-1] == 'zip':\n",
    "        path = e + 'zip/' +p.split('\\\\')[1].split('.')[0]\n",
    "        print(path)\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        with zipfile.ZipFile(p, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path)\n",
    "    if p.split('.')[-1] == 'gz':\n",
    "        path = e + 'gz/' +p.split('\\\\')[1].split('.')[0]\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        with gzip.open(p, 'rb') as f_in:\n",
    "            with open(path + '/' + p.split('\\\\')[1].split('.')[0]+'.tmx', 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare zip datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove readme & ids\n",
    "import os\n",
    "\n",
    "zips = './extracted/zip/*'\n",
    "paths = glob.glob(zips)\n",
    "\n",
    "for p in paths:\n",
    "    subs = glob.glob(p+'/*')\n",
    "    for s in subs:\n",
    "        if ('README' in s) or ('ids' in s):\n",
    "            os.remove(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change file names\n",
    "\n",
    "import os\n",
    "\n",
    "zips = './extracted/zip/*'\n",
    "paths = glob.glob(zips)\n",
    "\n",
    "for p in paths:\n",
    "    subs = glob.glob(p+'/*')\n",
    "    for s in subs:\n",
    "        s_new = p + '/' + s.split('.')[-1] + '.txt'\n",
    "        os.rename(s, s_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make csv from zips\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "zips = './problems/zip/*'\n",
    "paths = glob.glob(zips)\n",
    "\n",
    "problematics = []\n",
    "for p in paths:\n",
    "    subs = glob.glob(p+'/*.txt')\n",
    "    \n",
    "    slang = subs[0].split('\\\\')[-1].split('.')[0]\n",
    "    tlang = subs[1].split('\\\\')[-1].split('.')[0]\n",
    "    csv_name = p +'\\\\' + p.split('\\\\')[-1] + '.csv'\n",
    "\n",
    "    with open(subs[0], 'r', encoding='utf-8') as f:\n",
    "        # slang_data = f.read().splitlines()\n",
    "        slang_data = f.readlines()\n",
    "\n",
    "    with open(subs[1], 'r', encoding='utf-8') as f:\n",
    "        # tlang_data = f.read().splitlines()\n",
    "        tlang_data = f.readlines()\n",
    "    \n",
    "    tlang_data = [text.replace('\\n', '') for text in tlang_data]\n",
    "    slang_data = [text.replace('\\n', '') for text in slang_data]\n",
    "\n",
    "    try:\n",
    "        df = pd.DataFrame({slang: slang_data,\n",
    "                        tlang: tlang_data})\n",
    "        df.to_csv(csv_name, index=False)\n",
    "    except:\n",
    "        problematics.append({'slang':len(slang_data),\n",
    "                             'tlang': len(tlang_data),\n",
    "                             'slang_p': subs[0],\n",
    "                             'tlang_p': subs[1],\n",
    "                             'p':p})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare gz datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/amake/tmx2corpus.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "\n",
    "gzs = './problems/gz/*'\n",
    "paths = glob.glob(gzs)\n",
    "\n",
    "problematics = []\n",
    "for p in paths:\n",
    "    break\n",
    "    data = glob.glob(p+'/*')[0]\n",
    "    try:\n",
    "        os.system(\"tmx2corpus {}\".format(p))\n",
    "    except:\n",
    "        subs = glob.glob('./bitext*')\n",
    "        for s in subs:\n",
    "            os.remove(s)\n",
    "        problematics.append({p})\n",
    "\n",
    "    subs = glob.glob('./bitext*')\n",
    "    for s in subs:\n",
    "        print(s)\n",
    "        if \"tok\" not in s:\n",
    "            name = s.split('.')[-1] + '.txt'\n",
    "            os.rename(s, name)\n",
    "            Path(name).rename(p+ '\\\\' + name)\n",
    "        else:\n",
    "            os.remove(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make csv from gz\n",
    "\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "gzs = './problems/gz/*'\n",
    "paths = glob.glob(gzs)\n",
    "\n",
    "problematics = []\n",
    "for p in paths:\n",
    "    subs = glob.glob(p+'/*.txt')\n",
    "    \n",
    "    slang = subs[0].split('\\\\')[-1].split('.')[0]\n",
    "    tlang = subs[1].split('\\\\')[-1].split('.')[0]\n",
    "    csv_name = p +'\\\\' + p.split('\\\\')[-1] + '.csv'\n",
    "    with open(subs[0], 'r', encoding='utf-8') as f:\n",
    "        # slang_data = f.read().splitlines()\n",
    "        slang_data = f.readlines()\n",
    "\n",
    "    with open(subs[1], 'r', encoding='utf-8') as f:\n",
    "        # tlang_data = f.read().splitlines()\n",
    "        tlang_data = f.readlines()\n",
    "    \n",
    "    tlang_data = [text.replace('\\n', '') for text in tlang_data]\n",
    "    slang_data = [text.replace('\\n', '') for text in slang_data]\n",
    "\n",
    "    try:\n",
    "        df = pd.DataFrame({slang: slang_data,\n",
    "                        tlang: tlang_data})\n",
    "        df.to_csv(csv_name, index=False, encoding='utf-8')\n",
    "    except:\n",
    "        problematics.append({'slang':len(slang_data),\n",
    "                             'tlang': len(tlang_data),\n",
    "                             'slang_p': subs[0],\n",
    "                             'tlang_p': subs[1],\n",
    "                             'p':p})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "for i in range(len(slang_data)):\n",
    "    \n",
    "    if '\\n' in slang_data[i]:\n",
    "        idx+=1\n",
    "print(idx)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= 644903\n",
    "slang_data[index], tlang_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slang_data), len(tlang_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "\n",
    "dirs= ['gz', 'zip']\n",
    "csvs = './csvs/'\n",
    "for d in dirs:\n",
    "    gzs = './extracted/{}/*'.format(d)\n",
    "    paths = glob.glob(gzs)\n",
    "    for p in paths:\n",
    "        data = glob.glob(p+'/*.csv')\n",
    "        if len(data)>0:\n",
    "            data = data[0]\n",
    "            name = data.split('\\\\')[-1]\n",
    "            # print(name)\n",
    "\n",
    "            Path(data).rename(csvs+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./csvs/en-fa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not support\n",
    "# br, bs, he, hr, id\n",
    "# pt_br, tl, ze_en, ze_zh\n",
    "# zh_cn, zh_tw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "\n",
    "gzs = '../TEP/*'\n",
    "paths = glob.glob(gzs)\n",
    "\n",
    "problematics = []\n",
    "for p in paths:\n",
    "\n",
    "    try:\n",
    "        os.system(\"tmx2corpus {}\".format(p))\n",
    "    except:\n",
    "        subs = glob.glob('./bitext*')\n",
    "        for s in subs:\n",
    "            os.remove(s)\n",
    "        problematics.append({p})\n",
    "\n",
    "    subs = glob.glob('./bitext*')\n",
    "    for s in subs:\n",
    "        print(s)\n",
    "        if \"tok\" not in s:\n",
    "            name = s.split('.')[-1] + '.txt'\n",
    "            os.rename(s, name)\n",
    "            Path(name).rename(p.split('\\\\')[0] + '\\\\' +name)\n",
    "        else:\n",
    "            os.remove(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make csv from gz\n",
    "\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "gzs = '../TEP'\n",
    "paths = glob.glob(gzs)\n",
    "\n",
    "problematics = []\n",
    "for p in paths:\n",
    "    subs = glob.glob(p+'/*.txt')\n",
    "    slang = subs[0].split('\\\\')[-1].split('.')[0]\n",
    "    tlang = subs[1].split('\\\\')[-1].split('.')[0]\n",
    "    csv_name = p +'\\\\' + slang+'-'+tlang + '.csv'\n",
    "    with open(subs[0], 'r', encoding='utf-8') as f:\n",
    "        # slang_data = f.read().splitlines()\n",
    "        slang_data = f.readlines()\n",
    "\n",
    "    with open(subs[1], 'r', encoding='utf-8') as f:\n",
    "        # tlang_data = f.read().splitlines()\n",
    "        tlang_data = f.readlines()\n",
    "    \n",
    "    tlang_data = [text.replace('\\n', '') for text in tlang_data]\n",
    "    slang_data = [text.replace('\\n', '') for text in slang_data]\n",
    "\n",
    "    try:\n",
    "        df = pd.DataFrame({slang: slang_data,\n",
    "                        tlang: tlang_data})\n",
    "        print(csv_name)\n",
    "        df.to_csv(csv_name, index=False, encoding='utf-8')\n",
    "    except:\n",
    "        problematics.append({'slang':len(slang_data),\n",
    "                             'tlang': len(tlang_data),\n",
    "                             'slang_p': subs[0],\n",
    "                             'tlang_p': subs[1],\n",
    "                             'p':p})\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEP ++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "csv_name = '../TEP++/en-fa.csv'\n",
    "with open('../TEP++/fa.txt', 'r', encoding='utf-8') as f:\n",
    "    # slang_data = f.read().splitlines()\n",
    "    slang_data = f.readlines()\n",
    "\n",
    "with open('../TEP++/en.txt', 'r', encoding='utf-8') as f:\n",
    "    # tlang_data = f.read().splitlines()\n",
    "    tlang_data = f.readlines()\n",
    "\n",
    "tlang_data = [text.replace('\\n', '') for text in tlang_data]\n",
    "slang_data = [text.replace('\\n', '') for text in slang_data]\n",
    "\n",
    "df = pd.DataFrame({slang: slang_data,\n",
    "                tlang: tlang_data})\n",
    "df.to_csv(csv_name, index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEPC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "csv_name = '../PEPC_Bidirectional/en-fa.csv'\n",
    "fa_path = '../PEPC_Bidirectional/fa.txt'\n",
    "en_path = '../PEPC_Bidirectional/en.txt'\n",
    "\n",
    "csv_name = '../PEPC_Onedirectional/en-fa.csv'\n",
    "fa_path = '../PEPC_Onedirectional/fa.txt'\n",
    "en_path = '../PEPC_Onedirectional/en.txt'\n",
    "\n",
    "with open(fa_path, 'r', encoding='utf-8') as f:\n",
    "    # slang_data = f.read().splitlines()\n",
    "    slang_data = f.readlines()\n",
    "\n",
    "with open(en_path, 'r', encoding='utf-8') as f:\n",
    "    # tlang_data = f.read().splitlines()\n",
    "    tlang_data = f.readlines()\n",
    "\n",
    "tlang_data = [text.replace('\\n', '') for text in tlang_data]\n",
    "slang_data = [text.replace('\\n', '') for text in slang_data]\n",
    "\n",
    "df = pd.DataFrame({slang: slang_data,\n",
    "                tlang: tlang_data})\n",
    "df.to_csv(csv_name, index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_fa = pd.read_xml(\"../Bible/Farsi.xml\")\n",
    "df_en = pd.read_xml(\"../Bible/English.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"tmx2corpus {}\".format(\"../Bible/Farsi.xml\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mizan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pack your stuff.</td>\n",
       "      <td>وسايلتو جمع کن باشه.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I remember the cow that stayed over there.</td>\n",
       "      <td>يادمه يه گاو اونجا بود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_</td>\n",
       "      <td>و رييس سازمان \"ياماگاتو\" هم بين کشته ها هستن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oh</td>\n",
       "      <td>اِی #@%$* توش، غارِ یخـی؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aunt Silv, stop yelling!</td>\n",
       "      <td>عمه سيلو داد نزن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003995</th>\n",
       "      <td>Well, he did, and now he's here.</td>\n",
       "      <td>خب، اون زنده مونده و حالا هم اينجاست.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003996</th>\n",
       "      <td>The problem is, I can't trust you.</td>\n",
       "      <td>مشکل اینه که نمیتونم بهت اعتماد کنم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003997</th>\n",
       "      <td>Oh, and, uh, by the way...</td>\n",
       "      <td>...اوه و ضمناً</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003998</th>\n",
       "      <td>And that--that is on a timer.</td>\n",
       "      <td>و اون... اونم روي تايمره</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003999</th>\n",
       "      <td>'And Tom had to respect the fact that they wer...</td>\n",
       "      <td>در دست داشت و تام هم بايد اين حقيقت رو در نظر ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1004000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        en  \\\n",
       "0                                         Pack your stuff.   \n",
       "1               I remember the cow that stayed over there.   \n",
       "2                                                        _   \n",
       "3                                                       Oh   \n",
       "4                                 Aunt Silv, stop yelling!   \n",
       "...                                                    ...   \n",
       "1003995                   Well, he did, and now he's here.   \n",
       "1003996                 The problem is, I can't trust you.   \n",
       "1003997                         Oh, and, uh, by the way...   \n",
       "1003998                      And that--that is on a timer.   \n",
       "1003999  'And Tom had to respect the fact that they wer...   \n",
       "\n",
       "                                                        fa  \n",
       "0                                     وسايلتو جمع کن باشه.  \n",
       "1                                   يادمه يه گاو اونجا بود  \n",
       "2             و رييس سازمان \"ياماگاتو\" هم بين کشته ها هستن  \n",
       "3                                اِی #@%$* توش، غارِ یخـی؟  \n",
       "4                                         عمه سيلو داد نزن  \n",
       "...                                                    ...  \n",
       "1003995              خب، اون زنده مونده و حالا هم اينجاست.  \n",
       "1003996                مشکل اینه که نمیتونم بهت اعتماد کنم  \n",
       "1003997                                     ...اوه و ضمناً  \n",
       "1003998                           و اون... اونم روي تايمره  \n",
       "1003999  در دست داشت و تام هم بايد اين حقيقت رو در نظر ...  \n",
       "\n",
       "[1004000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/OpenSubtitles/en-fa.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make csv from gz\n",
    "\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "with open('./en-test.txt', 'r', encoding='utf-8') as f:\n",
    "    # slang_data = f.read().splitlines()\n",
    "    slang_data = f.readlines()\n",
    "\n",
    "with open('./fa-test.txt', 'r', encoding='utf-8') as f:\n",
    "    # tlang_data = f.read().splitlines()\n",
    "    tlang_data = f.readlines()\n",
    "\n",
    "tlang_data = [text.replace('\\n', '') for text in tlang_data]\n",
    "slang_data = [text.replace('\\n', '') for text in slang_data]\n",
    "\n",
    "df = pd.DataFrame({\"en\": slang_data,\n",
    "                \"fa\": tlang_data})\n",
    "df.to_csv('./test.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./train.csv\")\n",
    "df_tes = pd.read_csv(\"./test.csv\")\n",
    "df_dev = pd.read_csv(\"./dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_tes, df_dev])\n",
    "df.to_csv('./en-fa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "008b0f992822207dc3371086db865f2e3d549e6d0b94b092d243da12ff3366e0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('AI': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
